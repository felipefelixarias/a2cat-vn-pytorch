if __name__ == '__main__':
    import os,sys,inspect
    currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
    parentdir = os.path.dirname(os.path.dirname(currentdir))
    sys.path.insert(0,parentdir)

import tensorflow as tf
from keras.layers import Dense, Input, TimeDistributed
from keras import layers
from keras.models import Model, Sequential
from keras import initializers
from common import register_trainer, make_trainer
from a2c.a2c import A2CTrainer
import gym
import environment.qmaze


@register_trainer('test-a2c', episode_log_interval = 100, save = False)
class SomeTrainer(A2CTrainer):
    def __init__(self, **kwargs):
        super().__init__(env_kwargs = dict(id = 'CartPole-v0'), model_kwargs = dict(), **kwargs)


    def create_model(self, **model_kwargs):
        observation_space = self.env.observation_space
        action_space_size = self.env.action_space.n
        n_envs = self.n_envs

        mask = Input(batch_shape=(n_envs, None), name = 'rnn_mask')
        state_placeholders = []
        output_states = []

        def LSTM(number_of_units, **kwargs):
            layer = layers.LSTM(number_of_units,
                return_sequences = True, 
                return_state = True,
                **kwargs)

            def call(model):
                n_plh = len(state_placeholders)
                states = [Input((number_of_units,), name = 'rnn_state_%s' % (n_plh + i)) for i in range(2)]
                model, hidden, cell = layer(model, states, mask = mask)
                output_states.extend([hidden, cell])
                state_placeholders.extend(states)
                return model
            return call

        input_placeholder = Input(batch_shape=(n_envs, None) + observation_space.shape)
        policy_latent = LSTM(32)(input_placeholder)
        value_latent = LSTM(32)(input_placeholder)
    
        policy_probs = TimeDistributed(Dense(action_space_size, bias_initializer = 'zeros', activation='softmax', kernel_initializer = initializers.Orthogonal(gain=0.01)))(policy_latent)
        value = TimeDistributed(Dense(1, bias_initializer = 'zeros', kernel_initializer = initializers.Orthogonal(gain = 1.0)))(value_latent)

        model = Model(inputs = [input_placeholder, mask] + state_placeholders, outputs = [policy_probs, value] + output_states)
        return model

if __name__ == '__main__':
    t = make_trainer('test-a2c')
    t.run()

