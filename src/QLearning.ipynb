{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 0.,  0.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
    "    [ 1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_mark = 0.8  # Cells visited by the rat will be painted by gray 0.8\n",
    "rat_mark = 0.5      # The current rat cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "\n",
    "# Actions dictionary\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}\n",
    "\n",
    "num_actions = len(actions_dict)\n",
    "\n",
    "# Exploration factor\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maze is a 2d Numpy array of floats between 0.0 to 1.0\n",
    "# 1.0 corresponds to a free cell, and 0.0 an occupied cell\n",
    "# rat = (row, col) initial rat position (defaults to (0,0))\n",
    "\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, rat=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell where the \"cheese\" is\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells.remove(self.target)\n",
    "        if self._maze[self.target] == 0.0:\n",
    "            raise Exception(\"Invalid maze: target cell cannot be blocked!\")\n",
    "        if not rat in self.free_cells:\n",
    "            raise Exception(\"Invalid Rat Location: must sit on a free cell\")\n",
    "        self.reset(rat)\n",
    "\n",
    "    def reset(self, rat):\n",
    "        self.rat = rat\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = rat\n",
    "        self.maze[row, col] = rat_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = rat_row, rat_col, mode = self.state\n",
    "\n",
    "        if self.maze[rat_row, rat_col] > 0.0:\n",
    "            self.visited.add((rat_row, rat_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in rat position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        # new state\n",
    "        self.state = (nrow, ncol, nmode)\n",
    "\n",
    "    def get_reward(self):\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 1.0\n",
    "        if mode == 'blocked':\n",
    "            return self.min_reward - 1\n",
    "        if (rat_row, rat_col) in self.visited:\n",
    "            return -0.25\n",
    "        if mode == 'invalid':\n",
    "            return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        self.update_state(action)\n",
    "        reward = self.get_reward()\n",
    "        self.total_reward += reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the rat\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = rat_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        if self.total_reward < self.min_reward:\n",
    "            return 'lose'\n",
    "        rat_row, rat_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if rat_row == nrows-1 and rat_col == ncols-1:\n",
    "            return 'win'\n",
    "\n",
    "        return 'not_over'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    rat_row, rat_col, _ = qmaze.state\n",
    "    canvas[rat_row, rat_col] = 0.3   # rat cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # cheese cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='gray')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= -0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/tensorflow/lib/python3.6/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2807261da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAABb1JREFUeJzt3bFq02scx+FfDoKDLSg90MVRiHtyAenmlXgF2bwDSWfBK3AVLyC5gGRw7OZQkEIXUSeH/xmOwnE4J8lpfZvv6/NAt8D334QPbZf+RsMwFJDlj7t+AGB/woVAwoVAwoVAwoVAwoVAwoVAwoVAwoVA9/Z58dHR0XBycvKrnuUn3759q48fPzbZevr0aT148KDJ1tevX7vcar3X69aHDx/q+vp6tO11e4V7cnJSL168+P9PtYfPnz/XfD5vsvXq1auazWZNtlarVZdbrfd63ZpOpzu9zq/KEEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EGivf4j+6dOnevfu3a96lp+0/Efe3I7NZlNnZ2dNtpbLZZOdQzUahuG/XzAaPa+q51VVjx49mrx8+bLFc9Xx8XFdXl422RqPx3V0dNRk68uXL11uVVVdXV35zG5oPp/Xer2++QmSYRheV9XrqqqHDx8Ob9++vYXH2242mzU7QbJcLrs8Z9H6BMn5+bnPrBF/40Ig4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UIg4UKgvU6QPHnypNkJktVqVduuLNzmVq9Go63/FP/WLJfLZp/Z+fl5s3Mni8Xi4P4h+l4nSE5PTydv3rxp8VzdnupovXVxcdFkq6rtWZCW504eP35cp6enTbZ2PUFSwzDs/DWZTIZWlsulrVvYqqpmXy2/t8Vi0ez7WiwWzb6v741tbdHfuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBBIuBDICZI72Gp1FqTl6Yyqvj+zVltOkBzwVnV4OuPH92brZpwggY4JFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJFwIJt6o2m02NRqMmX5vNZq/rETf5mkwmd/3W8ou4HVRVV1dXdXl52WSr5T2flu9h671et9wO2sNisejynk/L97D1Xq9bbgdBx4QLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYQLgYRbVZPJpOlZkJbnTlpqfcql161dOEFyB1sXFxdNtlqeO6lqf8qlx635fF7DMDhBcohb1eG5k2Fof8qlx62/k3SCBLokXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAh0764fgH78OOXSwmq16nJrOp3u9DonSO5gq9cTJD1/Zq225vN5rddrJ0gOcas6PUHS82fWyvfGnCCBHgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAgkXAjlB0vlWq3MnVVXj8bjb9/H+/ftNtubzeb1//37rCZKt4f7TdDod1uv1jR5sV6vVqmazma0bbp2dnTXZqqpaLpfdvo/j8bjJ1rNnz3YK16/KEEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EEi4EGivEyRVNa6qVjct/qyqa1sxW633et0aD8NwvO1Fe50gaWk0Gq2HYZjaythqvfe7b/lVGQIJFwIdcrivbUVttd77rbcO9m9c4N8d8k9c4F8IFwIJFwIJFwIJFwL9BWUcjy5LKgWMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(maze)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f280716b6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, rat_cell):\n",
    "    qmaze.reset(rat_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_check(model, qmaze):\n",
    "    for cell in qmaze.free_cells:\n",
    "        if not qmaze.valid_actions(cell):\n",
    "            return False\n",
    "        if not play_game(model, qmaze, cell):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        # episode = [envstate, action, reward, envstate_next, game_over]\n",
    "        # memory[i] = episode\n",
    "        # envstate == flattened 1d maze cells info, including rat cell (see method: observe)\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = reward + self.discount * Q_sa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtrain(model, maze, **opt):\n",
    "    global epsilon\n",
    "    n_epoch = opt.get('n_epoch', 15000)\n",
    "    max_memory = opt.get('max_memory', 1000)\n",
    "    data_size = opt.get('data_size', 50)\n",
    "    weights_file = opt.get('weights_file', \"\")\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    # If you want to continue training from a previous model,\n",
    "    # just supply the h5 file name to weights_file option\n",
    "    if weights_file:\n",
    "        print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory)\n",
    "\n",
    "    win_history = []   # history of win/lose game\n",
    "    n_free_cells = len(qmaze.free_cells)\n",
    "    hsize = qmaze.maze.size//2   # history window size\n",
    "    win_rate = 0.0\n",
    "    imctr = 1\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = 0.0\n",
    "        rat_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(rat_cell)\n",
    "        game_over = False\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                win_history.append(1)\n",
    "                game_over = True\n",
    "            elif game_status == 'lose':\n",
    "                win_history.append(0)\n",
    "                game_over = True\n",
    "            else:\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "\n",
    "        if len(win_history) > hsize:\n",
    "            win_rate = sum(win_history[-hsize:]) / hsize\n",
    "    \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | Loss: {:.4f} | Episodes: {:d} | Win count: {:d} | Win rate: {:.3f} | time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, loss, n_episodes, sum(win_history), win_rate, t))\n",
    "        # we simply check if training has exhausted all free cells and if in all\n",
    "        # cases the agent won\n",
    "        if win_rate > 0.9 : epsilon = 0.05\n",
    "        if sum(win_history[-hsize:]) == hsize and completion_check(model, qmaze):\n",
    "            print(\"Reached 100%% win rate at epoch: %d\" % (epoch,))\n",
    "            break\n",
    "\n",
    "    # Save trained model weights and architecture, this will be used by the visualization code\n",
    "    h5file = name + \".h5\"\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "    return seconds\n",
    "\n",
    "# This is a small utility for printing readable time strings:\n",
    "def format_time(seconds):\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f seconds\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f minutes\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hours\" % (h,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(maze, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2807132f28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "maze =  np.array([\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  0.,  1.,  0.],\n",
    "    [ 0.,  0.,  0.,  1.,  1.,  1.,  0.],\n",
    "    [ 1.,  1.,  1.,  1.,  0.,  0.,  1.],\n",
    "    [ 1.,  0.,  0.,  0.,  1.,  1.,  1.],\n",
    "    [ 1.,  0.,  1.,  1.,  1.,  1.,  1.],\n",
    "    [ 1.,  1.,  1.,  0.,  1.,  1.,  1.]\n",
    "])\n",
    "\n",
    "qmaze = Qmaze(maze)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/14999 | Loss: 0.0022 | Episodes: 109 | Win count: 0 | Win rate: 0.000 | time: 4.4 seconds\n",
      "Epoch: 001/14999 | Loss: 0.0042 | Episodes: 109 | Win count: 0 | Win rate: 0.000 | time: 8.1 seconds\n",
      "Epoch: 002/14999 | Loss: 0.0034 | Episodes: 110 | Win count: 0 | Win rate: 0.000 | time: 11.5 seconds\n",
      "Epoch: 003/14999 | Loss: 0.0022 | Episodes: 102 | Win count: 0 | Win rate: 0.000 | time: 15.0 seconds\n",
      "Epoch: 004/14999 | Loss: 0.0337 | Episodes: 104 | Win count: 0 | Win rate: 0.000 | time: 19.1 seconds\n",
      "Epoch: 005/14999 | Loss: 0.0017 | Episodes: 106 | Win count: 0 | Win rate: 0.000 | time: 22.9 seconds\n",
      "Epoch: 006/14999 | Loss: 0.0023 | Episodes: 90 | Win count: 1 | Win rate: 0.000 | time: 26.2 seconds\n",
      "Epoch: 007/14999 | Loss: 0.0666 | Episodes: 103 | Win count: 1 | Win rate: 0.000 | time: 29.7 seconds\n",
      "Epoch: 008/14999 | Loss: 0.0759 | Episodes: 105 | Win count: 1 | Win rate: 0.000 | time: 34.4 seconds\n",
      "Epoch: 009/14999 | Loss: 0.0140 | Episodes: 3 | Win count: 2 | Win rate: 0.000 | time: 34.5 seconds\n",
      "Epoch: 010/14999 | Loss: 0.0053 | Episodes: 106 | Win count: 2 | Win rate: 0.000 | time: 38.4 seconds\n",
      "Epoch: 011/14999 | Loss: 0.0360 | Episodes: 1 | Win count: 3 | Win rate: 0.000 | time: 38.4 seconds\n",
      "Epoch: 012/14999 | Loss: 0.0029 | Episodes: 4 | Win count: 4 | Win rate: 0.000 | time: 38.6 seconds\n",
      "Epoch: 013/14999 | Loss: 0.0276 | Episodes: 104 | Win count: 4 | Win rate: 0.000 | time: 42.7 seconds\n",
      "Epoch: 014/14999 | Loss: 0.0032 | Episodes: 4 | Win count: 5 | Win rate: 0.000 | time: 42.9 seconds\n",
      "Epoch: 015/14999 | Loss: 0.0087 | Episodes: 50 | Win count: 6 | Win rate: 0.000 | time: 45.0 seconds\n",
      "Epoch: 016/14999 | Loss: 0.0025 | Episodes: 10 | Win count: 7 | Win rate: 0.000 | time: 45.5 seconds\n",
      "Epoch: 017/14999 | Loss: 0.0029 | Episodes: 30 | Win count: 8 | Win rate: 0.000 | time: 46.6 seconds\n",
      "Epoch: 018/14999 | Loss: 0.0317 | Episodes: 6 | Win count: 9 | Win rate: 0.000 | time: 46.8 seconds\n",
      "Epoch: 019/14999 | Loss: 0.0055 | Episodes: 2 | Win count: 10 | Win rate: 0.000 | time: 46.9 seconds\n",
      "Epoch: 020/14999 | Loss: 0.0223 | Episodes: 10 | Win count: 11 | Win rate: 0.000 | time: 47.3 seconds\n",
      "Epoch: 021/14999 | Loss: 0.0038 | Episodes: 45 | Win count: 12 | Win rate: 0.000 | time: 49.1 seconds\n",
      "Epoch: 022/14999 | Loss: 0.0050 | Episodes: 103 | Win count: 12 | Win rate: 0.000 | time: 53.0 seconds\n",
      "Epoch: 023/14999 | Loss: 0.0017 | Episodes: 7 | Win count: 13 | Win rate: 0.000 | time: 53.3 seconds\n",
      "Epoch: 024/14999 | Loss: 0.0030 | Episodes: 20 | Win count: 14 | Win rate: 0.583 | time: 54.2 seconds\n",
      "Epoch: 025/14999 | Loss: 0.0015 | Episodes: 6 | Win count: 15 | Win rate: 0.625 | time: 54.5 seconds\n",
      "Epoch: 026/14999 | Loss: 0.0009 | Episodes: 8 | Win count: 16 | Win rate: 0.667 | time: 54.8 seconds\n",
      "Epoch: 027/14999 | Loss: 0.0019 | Episodes: 5 | Win count: 17 | Win rate: 0.708 | time: 55.1 seconds\n",
      "Epoch: 028/14999 | Loss: 0.0011 | Episodes: 5 | Win count: 18 | Win rate: 0.750 | time: 55.3 seconds\n",
      "Epoch: 029/14999 | Loss: 0.0013 | Episodes: 4 | Win count: 19 | Win rate: 0.792 | time: 55.4 seconds\n",
      "Epoch: 030/14999 | Loss: 0.0034 | Episodes: 6 | Win count: 20 | Win rate: 0.792 | time: 55.7 seconds\n",
      "Epoch: 031/14999 | Loss: 0.0013 | Episodes: 4 | Win count: 21 | Win rate: 0.833 | time: 55.8 seconds\n",
      "Epoch: 032/14999 | Loss: 0.0033 | Episodes: 36 | Win count: 22 | Win rate: 0.875 | time: 57.3 seconds\n",
      "Epoch: 033/14999 | Loss: 0.0016 | Episodes: 1 | Win count: 23 | Win rate: 0.875 | time: 57.3 seconds\n",
      "Epoch: 034/14999 | Loss: 0.0017 | Episodes: 12 | Win count: 24 | Win rate: 0.917 | time: 57.8 seconds\n",
      "Epoch: 035/14999 | Loss: 0.0008 | Episodes: 14 | Win count: 25 | Win rate: 0.917 | time: 58.3 seconds\n",
      "Epoch: 036/14999 | Loss: 0.0017 | Episodes: 14 | Win count: 26 | Win rate: 0.917 | time: 58.9 seconds\n",
      "Epoch: 037/14999 | Loss: 0.0029 | Episodes: 1 | Win count: 27 | Win rate: 0.958 | time: 58.9 seconds\n",
      "Epoch: 038/14999 | Loss: 0.0007 | Episodes: 11 | Win count: 28 | Win rate: 0.958 | time: 59.4 seconds\n",
      "Epoch: 039/14999 | Loss: 0.0097 | Episodes: 77 | Win count: 29 | Win rate: 0.958 | time: 62.3 seconds\n",
      "Epoch: 040/14999 | Loss: 0.0016 | Episodes: 19 | Win count: 30 | Win rate: 0.958 | time: 63.0 seconds\n",
      "Epoch: 041/14999 | Loss: 0.0020 | Episodes: 14 | Win count: 31 | Win rate: 0.958 | time: 63.6 seconds\n",
      "Epoch: 042/14999 | Loss: 0.0018 | Episodes: 20 | Win count: 32 | Win rate: 0.958 | time: 64.3 seconds\n",
      "Epoch: 043/14999 | Loss: 0.0019 | Episodes: 64 | Win count: 33 | Win rate: 0.958 | time: 66.7 seconds\n",
      "Epoch: 044/14999 | Loss: 0.0016 | Episodes: 3 | Win count: 34 | Win rate: 0.958 | time: 66.8 seconds\n",
      "Epoch: 045/14999 | Loss: 0.0010 | Episodes: 20 | Win count: 35 | Win rate: 0.958 | time: 67.6 seconds\n",
      "Epoch: 046/14999 | Loss: 0.0002 | Episodes: 38 | Win count: 36 | Win rate: 1.000 | time: 69.1 seconds\n",
      "Reached 100% win rate at epoch: 46\n",
      "files: model.h5, model.json\n",
      "n_epoch: 46, max_mem: 392, data: 32, time: 69.8 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69.757203"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model(maze)\n",
    "qtrain(model, maze, epochs=500, max_memory=8*maze.size, data_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jonas/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input\n",
    "from keras.models import Sequential, Model\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from trfl import qlearning\n",
    "\n",
    "\n",
    "action_space_size = 2\n",
    "def create_inputs(name = 'main'):\n",
    "    return [Input(shape=(4,), name='%s_input' % name)]\n",
    "\n",
    "def create_model(action_space_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, activation='tanh'))\n",
    "    model.add(Dense(48, activation='tanh'))\n",
    "    model.add(Dense(action_space_size, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_loss(q, actions, rewards, pcontinues, qtarget):\n",
    "    return qlearning(q, actions, rewards, pcontinues, qtarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = tf.placeholder(tf.uint8, (None,))\n",
    "rewards = tf.placeholder(tf.float32, (None,))\n",
    "terminals = tf.placeholder(tf.bool, (None,))\n",
    "gamma = tf.placeholder_with_default(0.99, tuple())\n",
    "\n",
    "inputs = create_inputs()\n",
    "model_stream = create_model(action_space_size)\n",
    "q = model_stream(inputs)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = [q])\n",
    "\n",
    "# Create predict function\n",
    "model.predict_on_batch = K.function(inputs = inputs, outputs = [K.argmax(q, axis = 1)])\n",
    "\n",
    "# Next input targets\n",
    "next_step_inputs = create_inputs('next')\n",
    "next_q = K.stop_gradient(model_stream(next_step_inputs))\n",
    "\n",
    "# Build loss\n",
    "pcontinues = (1.0 - tf.to_float(terminals)) * gamma\n",
    "loss, _ = build_loss(q, actions, rewards, pcontinues, next_q)\n",
    "loss = K.mean(loss)\n",
    "\n",
    "\n",
    "# Build optimize\n",
    "optimizer = tf.train.AdamOptimizer(0.001)\n",
    "update = optimizer.minimize(loss)\n",
    "train_on_batch = K.Function(model.inputs + [actions, rewards, terminals] + next_step_inputs, [loss], updates = [update])\n",
    "model.train_on_batch = train_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepq.experiment import DeepQTrainer\n",
    "\n",
    "\n",
    "class Trainer(DeepQTrainer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dict(id = 'CartPole-v0'), dict(action_space_size = 2), annealing_steps = 10000, preprocess_steps = 10000, max_episode_steps = None)\n",
    "        self._minibatch_size = 64\n",
    "        pass\n",
    "    \n",
    "    def _wrap_env(self, env):\n",
    "        return env\n",
    "    \n",
    "    def _create_model(self, model_kwargs):\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 2)                 1418      \n",
      "=================================================================\n",
      "Total params: 1,418\n",
      "Trainable params: 1,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "steps: 176, episodes: 10, reward: 17.60000, episode length: 17.6, loss: nan\n",
      "steps: 395, episodes: 20, reward: 21.90000, episode length: 21.9, loss: nan\n",
      "steps: 572, episodes: 30, reward: 17.70000, episode length: 17.7, loss: nan\n",
      "steps: 823, episodes: 40, reward: 25.10000, episode length: 25.1, loss: nan\n",
      "steps: 1008, episodes: 50, reward: 18.50000, episode length: 18.5, loss: nan\n",
      "steps: 1227, episodes: 60, reward: 21.90000, episode length: 21.9, loss: nan\n",
      "steps: 1408, episodes: 70, reward: 18.10000, episode length: 18.1, loss: nan\n",
      "steps: 1619, episodes: 80, reward: 21.10000, episode length: 21.1, loss: nan\n",
      "steps: 1837, episodes: 90, reward: 21.80000, episode length: 21.8, loss: nan\n",
      "steps: 2021, episodes: 100, reward: 18.40000, episode length: 18.4, loss: nan\n",
      "steps: 2233, episodes: 110, reward: 21.20000, episode length: 21.2, loss: nan\n",
      "steps: 2542, episodes: 120, reward: 30.90000, episode length: 30.9, loss: nan\n",
      "steps: 2733, episodes: 130, reward: 19.10000, episode length: 19.1, loss: nan\n",
      "steps: 2981, episodes: 140, reward: 24.80000, episode length: 24.8, loss: nan\n",
      "steps: 3168, episodes: 150, reward: 18.70000, episode length: 18.7, loss: nan\n",
      "steps: 3376, episodes: 160, reward: 20.80000, episode length: 20.8, loss: nan\n",
      "steps: 3595, episodes: 170, reward: 21.90000, episode length: 21.9, loss: nan\n",
      "steps: 3819, episodes: 180, reward: 22.40000, episode length: 22.4, loss: nan\n",
      "steps: 4063, episodes: 190, reward: 24.40000, episode length: 24.4, loss: nan\n",
      "steps: 4299, episodes: 200, reward: 23.60000, episode length: 23.6, loss: nan\n",
      "steps: 4448, episodes: 210, reward: 14.90000, episode length: 14.9, loss: nan\n",
      "steps: 4705, episodes: 220, reward: 25.70000, episode length: 25.7, loss: nan\n",
      "steps: 4946, episodes: 230, reward: 24.10000, episode length: 24.1, loss: nan\n",
      "steps: 5168, episodes: 240, reward: 22.20000, episode length: 22.2, loss: nan\n",
      "steps: 5359, episodes: 250, reward: 19.10000, episode length: 19.1, loss: nan\n",
      "steps: 5553, episodes: 260, reward: 19.40000, episode length: 19.4, loss: nan\n",
      "steps: 5830, episodes: 270, reward: 27.70000, episode length: 27.7, loss: nan\n",
      "steps: 6050, episodes: 280, reward: 22.00000, episode length: 22.0, loss: nan\n",
      "steps: 6283, episodes: 290, reward: 23.30000, episode length: 23.3, loss: nan\n",
      "steps: 6535, episodes: 300, reward: 25.20000, episode length: 25.2, loss: nan\n",
      "steps: 6706, episodes: 310, reward: 17.10000, episode length: 17.1, loss: nan\n",
      "steps: 6952, episodes: 320, reward: 24.60000, episode length: 24.6, loss: nan\n",
      "steps: 7253, episodes: 330, reward: 30.10000, episode length: 30.1, loss: nan\n",
      "steps: 7448, episodes: 340, reward: 19.50000, episode length: 19.5, loss: nan\n",
      "steps: 7662, episodes: 350, reward: 21.40000, episode length: 21.4, loss: nan\n",
      "steps: 7917, episodes: 360, reward: 25.50000, episode length: 25.5, loss: nan\n",
      "steps: 8096, episodes: 370, reward: 17.90000, episode length: 17.9, loss: nan\n",
      "steps: 8358, episodes: 380, reward: 26.20000, episode length: 26.2, loss: nan\n",
      "steps: 8583, episodes: 390, reward: 22.50000, episode length: 22.5, loss: nan\n",
      "steps: 8810, episodes: 400, reward: 22.70000, episode length: 22.7, loss: nan\n",
      "steps: 9040, episodes: 410, reward: 23.00000, episode length: 23.0, loss: nan\n",
      "steps: 9328, episodes: 420, reward: 28.80000, episode length: 28.8, loss: nan\n",
      "steps: 9575, episodes: 430, reward: 24.70000, episode length: 24.7, loss: nan\n",
      "steps: 9761, episodes: 440, reward: 18.60000, episode length: 18.6, loss: nan\n",
      "steps: 9983, episodes: 450, reward: 22.20000, episode length: 22.2, loss: nan\n",
      "steps: 10184, episodes: 460, reward: 20.10000, episode length: 20.1, loss: 1.10899, epsilon:0.982\n",
      "steps: 10450, episodes: 470, reward: 26.60000, episode length: 26.6, loss: 5.24433, epsilon:0.956\n",
      "steps: 10666, episodes: 480, reward: 21.60000, episode length: 21.6, loss: 6.29513, epsilon:0.934\n",
      "steps: 10873, episodes: 490, reward: 20.70000, episode length: 20.7, loss: 7.54172, epsilon:0.914\n",
      "steps: 11115, episodes: 500, reward: 24.20000, episode length: 24.2, loss: 6.41384, epsilon:0.890\n",
      "steps: 11332, episodes: 510, reward: 21.70000, episode length: 21.7, loss: 5.01175, epsilon:0.868\n",
      "steps: 11738, episodes: 520, reward: 40.60000, episode length: 40.6, loss: 4.27770, epsilon:0.828\n",
      "steps: 12206, episodes: 530, reward: 46.80000, episode length: 46.8, loss: 8.86954, epsilon:0.782\n",
      "steps: 12678, episodes: 540, reward: 47.20000, episode length: 47.2, loss: 7.03357, epsilon:0.735\n",
      "steps: 13052, episodes: 550, reward: 37.40000, episode length: 37.4, loss: 7.39697, epsilon:0.698\n",
      "steps: 13595, episodes: 560, reward: 54.30000, episode length: 54.3, loss: 22.37548, epsilon:0.644\n",
      "steps: 13916, episodes: 570, reward: 32.10000, episode length: 32.1, loss: 11.60861, epsilon:0.612\n",
      "steps: 14321, episodes: 580, reward: 40.50000, episode length: 40.5, loss: 6.48231, epsilon:0.572\n",
      "steps: 14799, episodes: 590, reward: 47.80000, episode length: 47.8, loss: 10.85544, epsilon:0.525\n",
      "steps: 15358, episodes: 600, reward: 55.90000, episode length: 55.9, loss: 17.34440, epsilon:0.470\n",
      "steps: 15680, episodes: 610, reward: 32.20000, episode length: 32.2, loss: 28.59291, epsilon:0.438\n",
      "steps: 16201, episodes: 620, reward: 52.10000, episode length: 52.1, loss: 24.88369, epsilon:0.386\n",
      "steps: 16655, episodes: 630, reward: 45.40000, episode length: 45.4, loss: 22.86883, epsilon:0.341\n",
      "steps: 17348, episodes: 640, reward: 69.30000, episode length: 69.3, loss: 20.51990, epsilon:0.273\n",
      "steps: 17953, episodes: 650, reward: 60.50000, episode length: 60.5, loss: 10.25432, epsilon:0.213\n",
      "steps: 18934, episodes: 660, reward: 98.10000, episode length: 98.1, loss: 3.03507, epsilon:0.116\n",
      "steps: 20028, episodes: 670, reward: 109.40000, episode length: 109.4, loss: 21.55571, epsilon:0.010\n",
      "steps: 21740, episodes: 680, reward: 171.20000, episode length: 171.2, loss: 17.64482, epsilon:0.010\n",
      "steps: 23533, episodes: 690, reward: 179.30000, episode length: 179.3, loss: 7.69100, epsilon:0.010\n",
      "steps: 25288, episodes: 700, reward: 175.50000, episode length: 175.5, loss: 12.10473, epsilon:0.010\n",
      "steps: 27083, episodes: 710, reward: 179.50000, episode length: 179.5, loss: 5.61853, epsilon:0.010\n",
      "steps: 28520, episodes: 720, reward: 143.70000, episode length: 143.7, loss: 21.18695, epsilon:0.010\n",
      "steps: 29975, episodes: 730, reward: 145.50000, episode length: 145.5, loss: 9.95592, epsilon:0.010\n",
      "steps: 31431, episodes: 740, reward: 145.60000, episode length: 145.6, loss: 10.06285, epsilon:0.010\n",
      "steps: 32882, episodes: 750, reward: 145.10000, episode length: 145.1, loss: 19.32820, epsilon:0.010\n",
      "steps: 34514, episodes: 760, reward: 163.20000, episode length: 163.2, loss: 1.37444, epsilon:0.010\n",
      "steps: 35692, episodes: 770, reward: 117.80000, episode length: 117.8, loss: 20.27769, epsilon:0.010\n",
      "steps: 36948, episodes: 780, reward: 125.60000, episode length: 125.6, loss: 20.30799, epsilon:0.010\n",
      "steps: 38677, episodes: 790, reward: 172.90000, episode length: 172.9, loss: 12.20043, epsilon:0.010\n",
      "steps: 40381, episodes: 800, reward: 170.40000, episode length: 170.4, loss: 30.03895, epsilon:0.010\n",
      "steps: 41770, episodes: 810, reward: 138.90000, episode length: 138.9, loss: 5.93723, epsilon:0.010\n",
      "steps: 43569, episodes: 820, reward: 179.90000, episode length: 179.9, loss: 2.45057, epsilon:0.010\n",
      "steps: 44893, episodes: 830, reward: 132.40000, episode length: 132.4, loss: 34.78308, epsilon:0.010\n",
      "steps: 46352, episodes: 840, reward: 145.90000, episode length: 145.9, loss: 7.47121, epsilon:0.010\n",
      "steps: 47794, episodes: 850, reward: 144.20000, episode length: 144.2, loss: 6.42660, epsilon:0.010\n",
      "steps: 49042, episodes: 860, reward: 124.80000, episode length: 124.8, loss: 11.62722, epsilon:0.010\n",
      "steps: 50323, episodes: 870, reward: 128.10000, episode length: 128.1, loss: 12.42600, epsilon:0.010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 51645, episodes: 880, reward: 132.20000, episode length: 132.2, loss: 26.97726, epsilon:0.010\n",
      "steps: 53014, episodes: 890, reward: 136.90000, episode length: 136.9, loss: 1.46155, epsilon:0.010\n",
      "steps: 54490, episodes: 900, reward: 147.60000, episode length: 147.6, loss: 11.54817, epsilon:0.010\n",
      "steps: 55861, episodes: 910, reward: 137.10000, episode length: 137.1, loss: 17.46752, epsilon:0.010\n",
      "steps: 57569, episodes: 920, reward: 170.80000, episode length: 170.8, loss: 6.81042, epsilon:0.010\n",
      "steps: 58996, episodes: 930, reward: 142.70000, episode length: 142.7, loss: 15.32873, epsilon:0.010\n",
      "steps: 60602, episodes: 940, reward: 160.60000, episode length: 160.6, loss: 15.30395, epsilon:0.010\n",
      "steps: 62530, episodes: 950, reward: 192.80000, episode length: 192.8, loss: 5.48398, epsilon:0.010\n",
      "steps: 64442, episodes: 960, reward: 191.20000, episode length: 191.2, loss: 8.68874, epsilon:0.010\n",
      "steps: 66277, episodes: 970, reward: 183.50000, episode length: 183.5, loss: 3.95617, epsilon:0.010\n",
      "steps: 68090, episodes: 980, reward: 181.30000, episode length: 181.3, loss: 21.35714, epsilon:0.010\n",
      "steps: 70080, episodes: 990, reward: 199.00000, episode length: 199.0, loss: 7.33184, epsilon:0.010\n",
      "steps: 71936, episodes: 1000, reward: 185.60000, episode length: 185.6, loss: 26.32026, epsilon:0.010\n",
      "steps: 73778, episodes: 1010, reward: 184.20000, episode length: 184.2, loss: 3.22860, epsilon:0.010\n",
      "steps: 75601, episodes: 1020, reward: 182.30000, episode length: 182.3, loss: 13.60905, epsilon:0.010\n",
      "steps: 77497, episodes: 1030, reward: 189.60000, episode length: 189.6, loss: 1.03146, epsilon:0.010\n",
      "steps: 79389, episodes: 1040, reward: 189.20000, episode length: 189.2, loss: 7.70653, epsilon:0.010\n",
      "steps: 81323, episodes: 1050, reward: 193.40000, episode length: 193.4, loss: 1.31719, epsilon:0.010\n",
      "steps: 83112, episodes: 1060, reward: 178.90000, episode length: 178.9, loss: 2.66967, epsilon:0.010\n",
      "steps: 85022, episodes: 1070, reward: 191.00000, episode length: 191.0, loss: 19.40915, epsilon:0.010\n",
      "steps: 86862, episodes: 1080, reward: 184.00000, episode length: 184.0, loss: 8.38171, epsilon:0.010\n",
      "steps: 88634, episodes: 1090, reward: 177.20000, episode length: 177.2, loss: 6.77560, epsilon:0.010\n",
      "steps: 90086, episodes: 1100, reward: 145.20000, episode length: 145.2, loss: 23.45205, epsilon:0.010\n",
      "steps: 91365, episodes: 1110, reward: 127.90000, episode length: 127.9, loss: 2.70664, epsilon:0.010\n",
      "steps: 93118, episodes: 1120, reward: 175.30000, episode length: 175.3, loss: 5.91147, epsilon:0.010\n",
      "steps: 94515, episodes: 1130, reward: 139.70000, episode length: 139.7, loss: 5.05153, epsilon:0.010\n",
      "steps: 95320, episodes: 1140, reward: 80.50000, episode length: 80.5, loss: 20.36652, epsilon:0.010\n",
      "steps: 96886, episodes: 1150, reward: 156.60000, episode length: 156.6, loss: 33.25336, epsilon:0.010\n",
      "steps: 98130, episodes: 1160, reward: 124.40000, episode length: 124.4, loss: 21.02549, epsilon:0.010\n",
      "steps: 99577, episodes: 1170, reward: 144.70000, episode length: 144.7, loss: 6.06184, epsilon:0.010\n",
      "steps: 101372, episodes: 1180, reward: 179.50000, episode length: 179.5, loss: 13.01902, epsilon:0.010\n",
      "steps: 103372, episodes: 1190, reward: 200.00000, episode length: 200.0, loss: 6.55877, epsilon:0.010\n",
      "steps: 105362, episodes: 1200, reward: 199.00000, episode length: 199.0, loss: 16.85519, epsilon:0.010\n",
      "steps: 107247, episodes: 1210, reward: 188.50000, episode length: 188.5, loss: 16.79134, epsilon:0.010\n",
      "steps: 109247, episodes: 1220, reward: 200.00000, episode length: 200.0, loss: 11.12438, epsilon:0.010\n",
      "steps: 111232, episodes: 1230, reward: 198.50000, episode length: 198.5, loss: 11.70326, epsilon:0.010\n",
      "steps: 113223, episodes: 1240, reward: 199.10000, episode length: 199.1, loss: 12.07159, epsilon:0.010\n",
      "steps: 115170, episodes: 1250, reward: 194.70000, episode length: 194.7, loss: 14.06725, epsilon:0.010\n",
      "steps: 117170, episodes: 1260, reward: 200.00000, episode length: 200.0, loss: 20.92027, epsilon:0.010\n",
      "steps: 119107, episodes: 1270, reward: 193.70000, episode length: 193.7, loss: 11.50358, epsilon:0.010\n",
      "steps: 121107, episodes: 1280, reward: 200.00000, episode length: 200.0, loss: 9.52919, epsilon:0.010\n",
      "steps: 123100, episodes: 1290, reward: 199.30000, episode length: 199.3, loss: 14.29872, epsilon:0.010\n",
      "steps: 124954, episodes: 1300, reward: 185.40000, episode length: 185.4, loss: 16.62811, epsilon:0.010\n",
      "steps: 126744, episodes: 1310, reward: 179.00000, episode length: 179.0, loss: 10.24742, epsilon:0.010\n",
      "steps: 128622, episodes: 1320, reward: 187.80000, episode length: 187.8, loss: 10.90946, epsilon:0.010\n",
      "steps: 130592, episodes: 1330, reward: 197.00000, episode length: 197.0, loss: 30.03486, epsilon:0.010\n",
      "steps: 132537, episodes: 1340, reward: 194.50000, episode length: 194.5, loss: 10.38527, epsilon:0.010\n",
      "steps: 134502, episodes: 1350, reward: 196.50000, episode length: 196.5, loss: 17.73347, epsilon:0.010\n",
      "steps: 136496, episodes: 1360, reward: 199.40000, episode length: 199.4, loss: 21.24039, epsilon:0.010\n",
      "steps: 138352, episodes: 1370, reward: 185.60000, episode length: 185.6, loss: 15.09919, epsilon:0.010\n",
      "steps: 140250, episodes: 1380, reward: 189.80000, episode length: 189.8, loss: 3.45895, epsilon:0.010\n",
      "steps: 142250, episodes: 1390, reward: 200.00000, episode length: 200.0, loss: 18.05566, epsilon:0.010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0bb0242fb8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_log_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, process, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run is not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, process, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run is not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, process, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run is not implemented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, process)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mtdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mglobal_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train_wrappers.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/common/train_wrappers.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtdiff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_time_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/repos/target-driven-visual-navigation/src/deepq/experiment.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1434\u001b[0m         \u001b[0mrun_metadata_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0;31m# TODO(mrry): Switch to raising an exception from the SWIG wrapper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \u001b[0;34m\"\"\"Context manager to check for C API status.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from common.train_wrappers import wrap\n",
    "total_steps = 1000000\n",
    "trainer = Trainer()\n",
    "trainer = wrap(trainer, max_time_steps=total_steps, episode_log_interval=10)\n",
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
